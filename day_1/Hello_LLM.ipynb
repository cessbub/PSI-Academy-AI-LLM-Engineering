{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### AI/LLM Engineering Kick-off!! \n",
        "\n",
        "\n",
        "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, you'll need an OpenAI API Key. [here](https://platform.openai.com)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ecnJouXnUgKv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BzbKsZipLEs4pTx9dI97U6InAIB2G', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate building applications that leverage large language models (LLMs), but they serve different primary purposes and have distinct features. Here\\'s a high-level comparison:\\n\\n**1. Purpose and Focus**\\n\\n- **LangChain:**  \\n  Primarily a framework for building, managing, and deploying LLM-powered applications with a focus on composability, prompt management, and multi-step workflows. It provides tools for chains (sequence of calls), memory management, agents (dynamic decision making), and integrations with various APIs and data sources.\\n\\n- **LlamaIndex (GPT Index):**  \\n  Focused on enabling easy creation of indices over external data sources (like documents, PDFs, databases) so that LLMs can efficiently retrieve and reason over large, unstructured data. It simplifies data ingestion, indexing, and querying.\\n\\n**2. Core Functionality**\\n\\n- **LangChain:**  \\n  - Building \"chains\" of LLM calls (e.g., prompt + LLM + post-processing).  \\n  - Managing conversation state with memory modules.  \\n  - Creating agents that can decide on actions based on external tools or APIs.  \\n  - Supporting different LLM providers and making it easy to switch between them.  \\n  - Complex prompt engineering and template management.\\n\\n- **LlamaIndex:**  \\n  - Creating searchable indices over large datasets.  \\n  - Facilitating retrieval-augmented generation (RAG) workflows.  \\n  - Integrating various data sources and formats seamlessly.  \\n  - Providing simple APIs for indexing and querying that abstract away data retrieval complexities.\\n\\n**3. Use Cases**\\n\\n- **LangChain:**  \\n  - Conversational agents  \\n  - Multi-step reasoning tasks  \\n  - Complex workflows involving multiple prompts and API calls  \\n  - Building chatbots, virtual assistants, or automation pipelines\\n\\n- **LlamaIndex:**  \\n  - Document search and retrieval  \\n  - Building knowledge bases from unstructured data  \\n  - RAG systems for question answering over large datasets  \\n  - Data-driven applications where indexing large corpora is needed\\n\\n**4. Integration and Ecosystem**\\n\\n- **LangChain:**  \\n  - Broad integrations with LLM providers (OpenAI, Anthropic, Hugging Face, etc.)  \\n  - Supports multiple chains, tools, and custom components  \\n  - Active community and extensive documentation\\n\\n- **LlamaIndex:**  \\n  - Focused on data ingestion and retrieval  \\n  - Works well with various storage backends and data formats  \\n  - Can be combined with LLMs for advanced NLP tasks\\n\\n**Summary Table**\\n\\n| Aspect                     | LangChain                                              | LlamaIndex (GPT Index)                               |\\n|----------------------------|--------------------------------------------------------|------------------------------------------------------|\\n| Primary Purpose            | Orchestrating multi-step LLM workflows and apps     | Building and querying indices over unstructured data |\\n| Focus                      | Chain management, conversation, prompt engineering    | Data ingestion, retrieval, knowledge bases          |\\n| Use Cases                  | Chatbots, automation, complex reasoning workflows    | Search, RAG, document QA                            |\\n| Integrations               | Multiple LLM providers, APIs, tools                   | Data formats, storage backends                     |\\n| Complexity                   | General-purpose framework for applications             | Specializes in data indexing and retrieval        |\\n\\n**In essence:**  \\n- Use **LangChain** if you\\'re building complex applications involving multi-turn conversations, tool usage, or orchestrating various LLM calls.  \\n- Use **LlamaIndex** if your goal is to ingest large datasets, build search indices, and perform efficient retrieval-based querying with LLM augmentation.\\n\\n---\\n\\nLet me know if you need further details or specific examples!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754021226, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=750, prompt_tokens=19, total_tokens=769, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Great question! LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate the development of language model applications, especially for tasks involving document ingestion, retrieval, and chatbot creation. However, they have different focuses, architectures, and use cases. Here's an overview of their main differences:\n",
              "\n",
              "**1. Purpose and Focus**\n",
              "\n",
              "- **LangChain:**\n",
              "  - A comprehensive framework primarily aimed at building applications that leverage large language models (LLMs) for complex workflows.\n",
              "  - Focuses on connecting LLMs with external tools, APIs, data sources, and managing conversational states.\n",
              "  - Supports multi-step reasoning, prompt management, memory, and chaining multiple operations together.\n",
              "  - Suitable for building chatbots, question-answering systems, and integrated AI applications.\n",
              "\n",
              "- **LlamaIndex (GPT Index):**\n",
              "  - Designed specifically for indexing and querying large document collections to enable efficient retrieval using LLMs.\n",
              "  - Focuses on building indices over unstructured data (text, PDFs, CSVs, etc.) to perform question-answering and information retrieval.\n",
              "  - Acts as a bridge between raw data sources and LLMs for context-aware responses.\n",
              "\n",
              "**2. Architecture and Components**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Provides high-level abstractions such as Chains, Agents, Prompts, Memory, and tools.\n",
              "  - Modular design allows for flexible composition of workflows.\n",
              "  - Supports multiple models, prompt templates, and external integrations.\n",
              "  \n",
              "- **LlamaIndex:**\n",
              "  - Focuses on data ingestion, embedding generation, indexing, and retrieval.\n",
              "  - Uses constructs like indices (e.g., GPTSimpleVectorIndex, HierarchicalIndex) to organize data.\n",
              "  - Integrates with vector databases and supports retrieval-augmented generation (RAG) workflows.\n",
              "\n",
              "**3. Use Cases**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Building chatbots with multi-turn conversations.\n",
              "  - Automating complex workflows involving third-party APIs.\n",
              "  - Managing stateful interactions and multi-step reasoning.\n",
              "  \n",
              "- **LlamaIndex:**\n",
              "  - Creating a document search or question-answering system.\n",
              "  - Building knowledge bases from unstructured data.\n",
              "  - Quick retrieval of relevant documents to provide context for LLMs.\n",
              "\n",
              "**4. Integration with LLMs**\n",
              "\n",
              "- **Both frameworks** facilitate interacting with LLMs like OpenAI's GPT models.\n",
              "- **LangChain** emphasizes chaining and logic flow, making it ideal for applications requiring dynamic interactions and tool use.\n",
              "- **LlamaIndex** emphasizes retrieval of relevant data before passing it to the LLM for answering.\n",
              "\n",
              "---\n",
              "\n",
              "**Summary Table:**\n",
              "\n",
              "| Aspect                | LangChain                                           | LlamaIndex (GPT Index)                          |\n",
              "|-----------------------|-----------------------------------------------------|------------------------------------------------|\n",
              "| Primary Focus         | Application workflows, chaining, tools, conversation | Data ingestion, indexing, retrieval for documents |\n",
              "| Use Cases             | Chatbots, multi-step workflows, integrations       | Document QA, knowledge bases, info retrieval |\n",
              "| Architecture          | Modular chains, agents, prompts, memory            | Data indexing, vector retrieval, embedding management |\n",
              "| Data Handling         | External API integrations, multi-modal data       | Large document collections, unstructured data |\n",
              "| Interaction with LLM | Workflow orchestration, prompting, reasoning       | Retrieval augmented generation, context provision |\n",
              "\n",
              "---\n",
              "\n",
              "**In brief:**\n",
              "\n",
              "- **Use LangChain** if you're building complex conversational AI, applying reasoning, or orchestrating multiple tools and data sources.\n",
              "- **Use LlamaIndex** if your goal is to build a system that efficiently indexes large document repositories and enables quick, accurate retrieval and question-answering over unstructured data.\n",
              "\n",
              "Let me know if you'd like more detail on how to use either of these frameworks!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you kidding me? I don't have time to waste on petty ice preferences when I'm starving and furious about it! Just give me whatever is available—crushed, cubed, it doesn't matter—just get me something to eat already!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice has a fun, refreshing crunch that’s perfect for drinks like cocktails or snow cones, while cubed ice keeps beverages colder longer and is great for sipping. Both have their charm—depends on the mood! Which do you prefer?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-BzbLSnxD9mHn5laTN4vcrTEHi6hTZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I think crushed ice has a fun, refreshing crunch that’s perfect for drinks like cocktails or snow cones, while cubed ice keeps beverages colder longer and is great for sipping. Both have their charm—depends on the mood! Which do you prefer?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754021262, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=50, prompt_tokens=30, total_tokens=80, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Prompt Engineering\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Climate change refers to long-term shifts in temperature, precipitation, and other atmospheric patterns caused primarily by human activities such as burning fossil fuels, deforestation, and industrial processes. These activities increase greenhouse gases like carbon dioxide in the atmosphere, leading to global warming. The impacts of climate change include rising sea levels, more frequent and severe weather events, melting glaciers, and disruptions to ecosystems and agriculture. Addressing climate change requires collective actions to reduce emissions, transition to renewable energy sources, and promote sustainable practices worldwide."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Ay, nako, mga kaibigan! Alam nyo, kahit saan tayo pumunta, napapansin ko, ang usapin ng climate change, e, parang trending na rin—pero hindi maganda ang meaning, ha! Kasi itong climate change, parang nagkakalat na hindi natin naiiwasan, gaya ng uso—pero ito, hindi uso, seryoso ito! Umiinit, tumataas ang tubig baha, tagtuyot, at sakit sa panahon, parang nakikiusap na ang mundo na mag-ingat tayo. Kailangan talaga natin maging responsible, mag-recycle, mag-ipon ng lakas para sa kinabukasan ng planeta. Dahil kung hindi, baka bukas, wala na tayong mapuntahan kundi ang... climate apocalypse! Kaya mga kababayan, sama-sama tayo, magkaisa at maging love and care sa Earth natin. Lagi nating tandaan, ang pagbabago ay nagsisimula sa atin—kaya simulan na natin ngayon!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change as vice ganda in a talk show.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple wrench easily turned the falbean bolt, securing the parts tightly."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There are 2 r's in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry?\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "Notice that the model cannot count properly. It counted only 2 r's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #2: Update the prompt so that it can count correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There are 3 \"r\"s in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry?\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem),\n",
        "    assistant_prompt(\"In error, we split it into e-r-r-o-r and we can count that the string has 3 r's\"),\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Materials adapted for PSI AI Academy. Original materials from AI Makerspace."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
