{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Date received\", \n",
        "      \"Product\", \n",
        "      \"Sub-product\", \n",
        "      \"Issue\", \n",
        "      \"Sub-issue\", \n",
        "      \"Consumer complaint narrative\", \n",
        "      \"Company public response\", \n",
        "      \"Company\", \n",
        "      \"State\", \n",
        "      \"ZIP code\", \n",
        "      \"Tags\", \n",
        "      \"Consumer consent provided?\", \n",
        "      \"Submitted via\", \n",
        "      \"Date sent to company\", \n",
        "      \"Company response to consumer\", \n",
        "      \"Timely response?\", \n",
        "      \"Consumer disputed?\", \n",
        "      \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loan_complaint_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    loan_complaint_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"LoanComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the complaints provided, appears to be dealing with the mismanagement and inaccuracies related to loan balances, interest calculations, and the handling of payments by servicers. Many complaints involve errors in loan balances, misapplied payments, incorrect reporting of account status, and problems with loan transfers or communications from servicers. Additionally, issues with unfair or predatory repayment practices and difficulty in obtaining accurate information are prevalent.\\n\\nIn summary, the most common issue is **mismanagement of student loan accounts, including errors in balances, interest, and payment application, along with poor communication from loan servicers.**'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Based on the provided complaints, yes, there are several complaints indicating that issues were not handled in a timely manner. Specifically, two complaints directly mention a failure to respond within the expected time frame:\\n\\n1. Complaint from 03/28/25 (Complaint ID: 12709087) about a loan application not being processed, where the response was marked as 'No' for timely response.\\n2. Complaint from 04/05/25 (Complaint ID: 12832400) regarding a complaint to the CFPB about Aidvantage, which also was marked as 'Yes' for timely response, but the ongoing issue suggests delayed actions.\\n\\nAdditionally, multiple complaints highlight long delays, lack of responses, or failure to resolve issues within reasonable or legal timeframes, such as the complaint from 04/14/25 (Complaint ID: 12973003) about auto-pay and account issues that persisted beyond promised timelines.\\n\\nTherefore, yes, several complaints not only experienced delays but in at least one case explicitly were not handled in a timely manner.\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for various reasons, including:\\n\\n1. **Accumulation of interest and inability to increase payments**: Borrowers often found that lowering their payments led to interest accumulating faster than they could pay it off, making the debt grow over time.\\n\\n2. **Financial hardship and job-related issues**: Many borrowers assumed they would secure jobs supporting loan repayment but faced stagnating wages, unemployment, or employment in low-paying or unstable jobs, making it hard to meet repayment obligations.\\n\\n3. **Mismanagement and lack of clear information from servicers**: Some borrowers experienced poor communication, unexpected transfer of loans without notice, and unclear interest calculations, leading to confusion and inability to keep up with payments.\\n\\n4. **Inability to qualify for forgiveness programs**: Many did not qualify for programs like PSLF or TLF, which could reduce their debt, leaving them with unmanageable payments.\\n\\n5. **Loan servicing issues**: Problems such as being placed in forbearance long-term, not having payments properly communicated or scheduled, or being unable to make payments due to servicers’ poor handling contributed to delinquency.\\n\\n6. **Unexpected charges or lack of notification**: Some borrowers were unaware when payments resumed after grace periods or when loans were transferred, leading to delinquency.\\n\\n7. **Economic factors and high interest rates**: Years of high interest, recessions, and economic downturns increased overall debt, often beyond what borrowers initially borrowed or could afford.\\n\\nOverall, the combination of economic hardship, poor communication from loan servicers, lack of transparent information, and structural issues within the loan management system contributed heavily to borrowers’ inability to pay back their loans.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be related to dealing with lenders or servicers, particularly issues with miscommunication, incorrect information, or problems with repayment and fees. Specifically, recurring issues include:\\n\\n- Disputes over fees charged and incorrect explanations about loan balances or terms\\n- Difficulty applying payments correctly or paying down principal\\n- Receiving inaccurate or bad information about loans or loan history\\n- Problems with loan repayment terms, interest calculations, or loan duration\\n\\nTherefore, the most common issue is problems related to dealing with lenders or servicers, especially involving miscommunication, incorrect information, and repayment difficulties.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, all of them include responses from the companies that were marked as \"Timely response?\": \"Yes.\" Therefore, it appears that all complaints were handled in a timely manner.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for various reasons, including issues with their payment plans, miscommunication or lack of communication from the loan servicers, being unaware of account status changes, or experiencing mistakes and complications caused by the lenders or servicers. For instance, some borrowers were not properly informed about their loan transfers, repayment schedule changes, or issues with autopay enrollment, leading to missed payments and negative credit impacts. Others faced problems with billing errors, incorrect account information, or prolonged delays in response from the loan servicing companies, which hindered their ability to make or track payments effectively.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "#### ❓ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "### Answer:\n",
        "* example query: \"\"Philippines Income Tax Act 1997 Section 45\"\n",
        "* This query is looking for a very specific law section. The exact wording and number (“Section 45”) matter a lot.\n",
        "* Legal references often have unique identifiers (numbers, subsections, letter codes) that are not semantically similar to anything else. Embedding models can miss their exactness because they compress meaning into a vector space where “Section 45” and “Section 44” might be close. BM25 treats them as totally different terms.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers, including receiving incorrect or bad information about loans, errors in loan balances, misapplied payments, wrongful denials of payment plans, and mishandling of loan data. Many complaints involve confusion over balances, unapproved transfers of loans, lack of communication, and disputes over accuracy and privacy violations.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, there is an indication that some complaints did not get handled in a timely manner. For example, the complaint from the individual regarding their student loans and account review has been open since back when the complaint was submitted (specific dates are redacted with XXXX), and it mentions that it has been nearly 18 months with no resolution. Additionally, the complaint from the person with payments totaling $3100 that were not applied to their account also references a timeline of over 2-3 weeks and ongoing issues. \\n\\nTherefore, yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a combination of factors including lack of understanding about repayment responsibilities, poor communication from lenders and servicers, and financial hardships. Specifically:\\n\\n- Many borrowers were unaware that they needed to repay their student loans, especially if they were not adequately informed by financial aid officers.\\n- There were issues with loan servicers failing to notify borrowers of payment due dates, changes in loan ownership, or terms of repayment.\\n- Borrowers often faced difficulties accessing accurate or consistent account information, leading to confusion about balances and interest.\\n- Restricted payment options like forbearance and deferment resulted in continued interest accumulation, making it harder to pay off the loans and sometimes increasing total debt.\\n- Some borrowers experienced increased balances despite payments due to accumulation of interest and a lack of clear, transparent breakdowns of their loan status.\\n- Financial hardships, stagnant wages, and the inability to afford increased payments or pay off accruing interest contributed to ongoing non-repayment.\\n\\nIn summary, a combination of miscommunication, lack of clarity, and financial challenges led many people to fail to keep up with their loan repayment obligations.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common issue with loans appears to be problems related to \"Dealing with your lender or servicer,\" which includes sub-issues such as errors in loan balances, misapplied payments, wrongful denials of payment plans, and bad information about loans. A recurring theme is mismanagement by loan servicers, inaccurate reporting, and difficulties in resolving disputes or clarifying account details.\\n\\nTherefore, the most common issue with loans from the given data is:\\n\\n**Problems with loan servicers, including mismanagement, incorrect information, and difficulties in communication or resolution.**'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, yes, some complaints indicate that they were not handled in a timely manner. Specifically:\\n\\n- Several complaints note delayed responses or lack of response from the companies, despite multiple follow-ups. For example:\\n  - Complaint from 04/18/25 (Complaint ID: 13062402) against Nelnet states that inaccuracies in reporting were not corrected within required timeframes, and disputes were not resolved promptly.\\n  - Complaint from 04/21/25 (Complaint ID: 13091395) against Maximus Federal Services reports that the company did not respond within the required 15 days.\\n  - Complaint from 04/27/25 (Complaint ID: 13205525) against Nelnet mentions that over 30 days had passed without response to a dispute.\\n  - Complaint from 05/08/25 (Complaint ID: 13410623) against Maximus Federal Services details that they waited over 18 months for resolution, and the complaint was inadequately addressed.\\n\\nThese examples suggest that, in multiple instances, complaints were not handled in a timely manner.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to factors such as mismanagement by loan servicers, inaccurate or delayed communication about payments, and the accumulation of interest during periods of forbearance or deferment. Many borrowers experienced issues like being steered into long-term forbearances with little notice about the consequences, or being unable to apply additional payments toward the principal, which prolonged their debt. Others faced errors in reporting their payment history or account status, which adversely affected their credit scores. Additionally, some borrowers were misled about repayment options, interest accrual, and loan forgiveness programs, making it difficult for them to manage and pay off their loans effectively. These systemic issues, combined with financial hardships and lack of transparent information, contributed to the failure of many individuals to pay back their loans.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "#### ❓ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Answer:\n",
        "\n",
        "- Generating multiple reformulations of a user query can improve recall because it helps the retriever find more relevant documents that might use different wording or phrasing than the original query.\n",
        "- This improves recall because it reduces the risk of missing relevant documents that use different words, structures, or terms from the original query.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = loan_complaint_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be related to incorrect or misleading information in the credit reporting process, such as errors in loan balances, account status, or interest rates, as well as issues with loan servicer misconduct, including misapplied payments, wrongful denials of payment plans, and disputes over debt legitimacy.'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, all four complaints indicate that they were not handled in a timely manner. Specifically, the first two complaints involving Mohela\\'s student loan servicing explicitly mention that responses or communications took longer than expected, and the complaints were marked as \"No\" for \"Timely response.\" \\n\\nTherefore, yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans due to several reasons highlighted in the complaints. These include:\\n\\n1. Lack of clear communication and proper notification from loan servicers about when payments were due, such as failing to inform borrowers about payment start dates or changes in loan ownership.\\n2. Challenges in managing repayment during financial hardship, such as experiencing severe financial difficulties, unemployment, or health issues, which made it difficult to make consistent payments.\\n3. Mismanagement or failures by institutions, including not reevaluating payments based on the borrower's circumstances, or reporting delinquency while issues were unresolved.\\n4. Misrepresentation or lack of transparency by educational institutions about the value of the degrees, career prospects, and the debt they encouraged students to take on, leading to financial hardship after graduation.\\n5. Issues related to loan servicers' practices, such as improper reporting of late payments, failure to verify legitimacy of debts, or inadequate notification about loan details.\\n\\nOverall, the combination of miscommunication, financial hardship, institutional mismanagement, and inadequate borrower support contributed to the failure to repay loans.\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, the most common issues with student loans appear to be:\\n\\n- Dealing with lenders or servicers issues, including errors in loan balances, misapplied payments, wrongful denials of payment plans, and trouble with how payments are handled.\\n- Problems arising from loan transfer and lack of clear communication about loan terms, balances, interest calculations, or transfers without proper notification.\\n- Incorrect or inconsistent information reported on credit reports and mismanagement leading to credit score drops.\\n- Lack of proper documentation (e.g., missing signed promissory notes) and failure of servicers to provide required legal loan documents.\\n- Issues related to loan repayment difficulties due to interest accumulation, improper loan consolidation handling, and problems with loan discharge or forgiveness programs.\\n- Unauthorized or inaccurate reporting and breach of privacy (FERPA violations).\\n\\nIn summary, the most common issue seems to be **mismanagement and improper handling of loans by servicers and lenders, including lack of transparency, errors in account information, and difficulties in communication and documentation**.'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints data, several complaints indicate that issues were not handled in a timely manner. Specifically:\\n\\n- Complaint ID 12973003 from EdFinancial Services (NJ) submitted on 04/14/25 reports that the company responded \"Yes\" to timely response, but the complaint states the issue had been ongoing for 2-3 weeks with no resolution, and the customer was still experiencing problems after waiting over 2-3 weeks, suggesting a delay.\\n\\n- Complaint ID 12654977 from MOHELA (MD) submitted on 03/25/25 indicates that Mohela\\'s response was \"No\" to timely response, implying they did not handle the issue promptly. The complaint mentions that payments were not being processed correctly for over a month, and multiple complaints and escalations to no avail.\\n\\n- Complaint ID 13062402 from Nelnet (PA) received on 04/18/25 shows the response was \"Yes\" to timely, but the complaint indicates the error persisted for over 10 days beyond the promised 48 hours, and the problem was not resolved.\\n\\n- Several other complaints (e.g., IDs 12832400, 13056764, 12950199, 13091395) show timely responses from the companies, but the complainants report ongoing unresolved issues or delays in correction.\\n\\nIn summary, while some complaints were responded to within the expected timeframe, many complaints detail that issues remained unresolved or were not addressed promptly, often taking several weeks beyond promised resolution times.\\n\\nTherefore, yes, there are complaints indicating that some issues were not handled in a timely manner.'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a combination of factors such as unmanageable interest accumulation, lack of clear communication and proper notification from loan servicers about payment obligations, misleading steering into forbearance or deferment options that led to increased debt, systemic mismanagement, and insufficient information about repayment options like income-driven plans or loan forgiveness programs. Additionally, some struggled because of financial hardships, employment issues, or misunderstandings about the status of their loans, including transfers between loan servicers and errors in reporting, all contributing to their inability to make timely payments.'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the complaints provided, appears to be problems related to loan servicing and handling. Specific issues include incorrect or delayed information about loan balances, repayment plans, or payment processing; improper reporting or errors on credit reports; and issues with loan forgiveness or discharge documentation. Many complaints also highlight a lack of communication, transparency, and accountability from loan servicers like Nelnet, EdFinancial, and Aidvantage.'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, it appears that several complaints were marked as handled in a timely manner, with responses noted as \"Yes\" for timely response and \"Closed with explanation\" for company responses. However, there is at least one complaint (Complaint ID: 13331376) where the consumer reported that despite the company response being \"Closed with explanation,\" the issues remained unresolved, and the consumer continued to face problems such as transfer of accounts, errors, and alleged misconduct.\\n\\nIn particular, the complaint regarding the transfer of a student loan to Nelnet (ID: 13331376) involved ongoing issues and allegations of misconduct, despite the company\\'s response indicating the complaint was closed. This suggests that some complaints may not have been fully handled to the consumer\\'s satisfaction or in a manner that resolved the underlying issues.\\n\\nTherefore, while the records indicate many complaints were marked as handled \"timely,\" there are indications that some complaints, such as the one mentioned above, may not have been fully resolved or handled in a manner satisfactory to the complainants.\\n\\nIn summary:  \\nYes, some complaints did not get fully handled in a timely or satisfactory manner, as evidenced by ongoing issues reported by consumers in certain complaints.'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, people failed to pay back their loans for various reasons, including:\\n\\n1. Lack of clear communication and transparency from lenders or servicers, leading to confusion about loan status and terms.\\n2. Difficulties in verifying or providing required documentation for loan forgiveness or discharge, resulting in delays and stalls.\\n3. Disputes over the legitimacy or accuracy of reported loan information, often due to errors or alleged improper reporting.\\n4. Technical issues or errors in payment processing, such as payments not being received or processed correctly.\\n5. Concerns over improper or illegal practices by loan servicers, including unauthorized access to personal data, or reporting debts that are now legally void.\\n6. Financial hardships or changes in personal circumstances, although not explicitly detailed in the complaints, can also be a factor.\\n\\nIn summary, failures to pay back loans often stem from administrative issues, disputes over loan legitimacy, communication problems, and legal or procedural complications.'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "#### ❓ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Answer:\n",
        "When sentences are short and highly repetitive, like in FAQs, semantic chunking often over-splits the text into tiny pieces, creating many near-duplicate chunks that can clutter the index and confuse retrieval. This can also separate questions from their answers, which reduces the usefulness of the results and may cause the retriever to return only partial context. To address this, I would adjust the algorithm to chunk by full FAQ pairs (question and answer together) instead of individual sentences, merge related FAQs in the same category until they reach a minimum token size (around 300–500 tokens), and keep only a small overlap of 20–50 tokens between merged groups to preserve context without creating redundancy. I’d also normalize and deduplicate entries by hashing the question text, filter near-duplicates during retrieval using maximal marginal relevance, and combine BM25 with embeddings for hybrid retrieval. Finally, I would include metadata like category, FAQ ID, and section headers in each chunk so that retrieval remains precise and contextually relevant.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against each other. \n",
        "You can use the loans or bills dataset.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded pages: 93 | files attempted: 8 | skipped: 0\n",
            "chunks: 214\n",
            "sample metadata: {'producer': '', 'creator': 'Image Capture Plus', 'creationdate': '2025-07-02T09:41:16+00:00', 'source': 'bills/20250725 SBN 25 AI Regulation Act.pdf', 'file_path': 'bills/20250725 SBN 25 AI Regulation Act.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-07-02T09:41:16+00:00', 'trapped': '', 'modDate': 'D:20250702094116Z', 'creationDate': 'D:20250702094116Z', 'page': 0, 'id': '20250725_SBN_25_AI_Regulation_Act.pdf#chunk_0'}\n",
            "note: no ground truth found for 'what is the separability clause of sbn 25?'\n",
            "note: no ground truth found for 'what is the repealing clause of sbn 25?'\n",
            "note: no ground truth found for 'what is the separability clause of sb 29?'\n",
            "note: no ground truth found for 'what is the repealing clause of sb 29?'\n",
            "note: no ground truth found for 'what is the declaration of policy under hb 7913?'\n",
            "note: no ground truth found for 'what are the definitions under hb 7913?'\n",
            "golden size now: 30\n",
            "golden preview: [{'question': 'what is the short title of sbn 25?', 'answer': 'TWENTIETH CONGRESS OF THE \\nREPUBLIC OF THE PHILIPPINES \\nFirst Regular Session\\n)\\nSENATE \\nS. No. 2.ci\\n25 JUL-2 P4:56\\nn;-\\' :\\nIntroduced by Senator PIA S. CAYETANO\\nAN ACT\\nREGULATING THE DEVELOPMENT AND USE OF ARTIFICIAL INTELLIGENCE \\nSYSTEMS IN THE PHILIPPINES, PROMOTING ETHICAL AND RESPONSIBLE \\nARTIFICIAL INTELLIGENCE INNOVATION, AND INTEGRATING \\nSUSTAINABILITY AND FUTURES THINKING IN NATIONAL POLICY MAKING, \\nAND FOR OTHER PURPOSES\\nBe it enacted by the Senate and House of Representatives of the Philippines in \\nCongress assembled:\\n1 \\nSection 1. Short Title. - This Act shall be known as the \\'Artificial Intelligence\\n2 \\nRegulation Act (AIRA).\"\\n3', 'ground_truth_doc_ids': ['20250725_SBN_25_AI_Regulation_Act.pdf#chunk_7']}, {'question': 'what is the declaration of policy under sbn 25?', 'answer': '2 \\nRegulation Act (AIRA).\"\\n3 \\nSec. 2. Declaration of Policy. - It is the policy of the State to recognize that\\n4 \\nscience and technology are essential for national development and progress. The State\\n5 \\nshall give priority to research and development, invention, innovation, and their\\n6 \\nutilization; and to science and technology education, training, and services.1 In this\\n7 \\nregard, the State shall promote the responsible development and use of Artificial\\n8 \\nIntelligence (AI) to advance inclusive growth, public service delivery, innovation, and\\n9 \\nlong-term national resilience. It shall likewise institutionalize futures thinking and\\n10', 'ground_truth_doc_ids': ['20250725_SBN_25_AI_Regulation_Act.pdf#chunk_8']}, {'question': 'what are the objectives under sbn 25?', 'answer': '10 \\nsustainability as core principles in education, governance, and innovation. Towards\\n11 \\nthis end, the State shall advance AI in a manner that is ethical, inclusive, transparent,\\n12 \\nand accountable.\\n13 \\nSec. 3. Objectives. - The objectives of this Act are the following:\\n1 Article XIV, Section 10 of the Philippine Constitution', 'ground_truth_doc_ids': ['20250725_SBN_25_AI_Regulation_Act.pdf#chunk_9']}, {'question': 'what is the coverage of sbn 25?', 'answer': '20 \\nare driven by futures thinking, strategic foresight, and proactive risk\\n21 \\nmanagement, and are governed by \\nprinciples \\nof transparency,\\n22 \\naccountability, and sustainability.\\n23 \\nSec. 4. Coverage. - This Act shall regulate the development and use of all types\\n24 \\nof AI, including Artificial General Intelligence (AGI) and Artificial Superintelligence\\n25 \\n(ASI), as well as regulate AI foundation models, such as machine learning systems,\\n26 \\ngenerative AI, neural networks, expert systems, language learned models (LLMs), and\\n27 \\nGenerative Pre-trained Transformers. This shall apply to all individuals, corporations,\\n28', 'ground_truth_doc_ids': ['20250725_SBN_25_AI_Regulation_Act.pdf#chunk_12']}, {'question': 'what are the definitions under sbn 25?', 'answer': '28 \\ninstitutions, and government agencies that develop, deploy, use or operate AI systems\\n29 \\nin the Philippines.\\n30 \\nSec. 5. Definition of Terms. - For the purposes of this Act:\\n31 \\na) Artificiai Inteiiigence (AI) - refers to systems that allow machines to\\n32 \\nthink like humans, such that they can display intelligent behavior by', 'ground_truth_doc_ids': ['20250725_SBN_25_AI_Regulation_Act.pdf#chunk_13']}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 60/60 [00:57<00:00,  1.04it/s]\n",
            "Evaluating:  72%|███████▏  | 43/60 [01:24<00:29,  1.73s/it]Exception raised in Job[56]: InternalServerError(upstream connect error or disconnect/reset before headers. reset reason: connection termination)\n",
            "Evaluating: 100%|██████████| 60/60 [02:28<00:00,  2.48s/it]\n",
            "Evaluating: 100%|██████████| 60/60 [01:38<00:00,  1.64s/it]\n",
            "Evaluating: 100%|██████████| 60/60 [01:09<00:00,  1.16s/it]\n",
            "Evaluating: 100%|██████████| 60/60 [01:40<00:00,  1.68s/it]\n",
            "Evaluating: 100%|██████████| 60/60 [00:49<00:00,  1.21it/s]\n",
            "Evaluating: 100%|██████████| 60/60 [00:32<00:00,  1.85it/s]\n",
            "Evaluating: 100%|██████████| 60/60 [00:43<00:00,  1.38it/s]\n",
            "Evaluating: 100%|██████████| 60/60 [00:45<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== retriever comparison (k = EVAL_K) ===\n",
            "        retriever  context_precision  context_recall  avg_latency_ms\n",
            "             bm25              0.415             1.0             0.6\n",
            "            dense              0.526             0.0            16.9\n",
            "           hybrid              0.594             0.0            16.2\n",
            "  bm25_smallchunk              0.800             0.0             0.9\n",
            " dense_smallchunk              0.962             0.0            16.8\n",
            "hybrid_smallchunk              1.000             0.0            17.5\n",
            "\n",
            "=== retriever comparison (k = EVAL_K_PREC) ===\n",
            "retriever  context_precision  context_recall  avg_latency_ms\n",
            " dense_k2                0.0           0.167            16.4\n",
            "  bm25_k2                0.0           0.000             0.6\n",
            "hybrid_k2                0.5           0.000            13.3\n",
            "\n",
            "=== quick summary (k = EVAL_K) ===\n",
            "- bm25: context_precision=0.415, context_recall=1.0 | avg_latency_ms=0.6\n",
            "- dense: context_precision=0.526, context_recall=0.0 | avg_latency_ms=16.9\n",
            "- hybrid: context_precision=0.594, context_recall=0.0 | avg_latency_ms=16.2\n",
            "- bm25_smallchunk: context_precision=0.8, context_recall=0.0 | avg_latency_ms=0.9\n",
            "- dense_smallchunk: context_precision=0.962, context_recall=0.0 | avg_latency_ms=16.8\n",
            "- hybrid_smallchunk: context_precision=1.0, context_recall=0.0 | avg_latency_ms=17.5\n",
            "\n",
            "=== quick summary (k = EVAL_K_PREC) ===\n",
            "- dense_k2: context_precision=0.0, context_recall=0.167 | avg_latency_ms=16.4\n",
            "- bm25_k2: context_precision=0.0, context_recall=0.0 | avg_latency_ms=0.6\n",
            "- hybrid_k2: context_precision=0.5, context_recall=0.0 | avg_latency_ms=13.3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import statistics\n",
        "import unicodedata\n",
        "from copy import deepcopy\n",
        "from typing import List, Tuple, Dict, Callable\n",
        "\n",
        "# config\n",
        "FOLDER = \"bills\"                 # change if needed\n",
        "EVAL_K = 8                       # main top-k for recall-oriented run\n",
        "EVAL_K_PREC = 2                  # low top-k secondary run to improve precision\n",
        "DO_SMALL_CHUNK_COMPARISON = True # set to False if you do not want the small-chunk run\n",
        "\n",
        "# -----------------------\n",
        "# load and chunk documents\n",
        "# -----------------------\n",
        "\n",
        "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader, PDFPlumberLoader\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "pdf_files = sorted([f for f in os.listdir(FOLDER) if f.lower().endswith(\".pdf\")])\n",
        "\n",
        "def load_pdf_any(path: str) -> List[Document]:\n",
        "    # try pymupdf first\n",
        "    try:\n",
        "        return PyMuPDFLoader(path).load()\n",
        "    except Exception:\n",
        "        pass\n",
        "    # then pypdf\n",
        "    try:\n",
        "        return PyPDFLoader(path).load()\n",
        "    except Exception:\n",
        "        pass\n",
        "    # finally pdfplumber\n",
        "    try:\n",
        "        return PDFPlumberLoader(path).load()\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "docs = []\n",
        "skipped = []\n",
        "for fname in pdf_files:\n",
        "    path = os.path.join(FOLDER, fname)\n",
        "    got = load_pdf_any(path)\n",
        "    if got:\n",
        "        docs.extend(got)\n",
        "    else:\n",
        "        skipped.append(fname)\n",
        "\n",
        "print(f\"loaded pages: {len(docs)} | files attempted: {len(pdf_files)} | skipped: {len(skipped)}\")\n",
        "if skipped:\n",
        "    print(\"skipped files:\", skipped[:10])\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# use a legal-text-friendly chunk size\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=80)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "# add stable ids\n",
        "for i, d in enumerate(chunks):\n",
        "    base = d.metadata.get(\"source\", f\"src_{i}\").split(\"/\")[-1].replace(\" \", \"_\")\n",
        "    d.metadata[\"id\"] = f\"{base}#chunk_{i}\"\n",
        "\n",
        "print(\"chunks:\", len(chunks))\n",
        "print(\"sample metadata:\", chunks[0].metadata if chunks else \"no chunks\")\n",
        "\n",
        "# -----------------------\n",
        "# build retrievers: bm25, dense/faiss, hybrid\n",
        "# -----------------------\n",
        "\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "bm25 = BM25Retriever.from_documents(chunks)\n",
        "bm25.k = EVAL_K\n",
        "\n",
        "hf_emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "faiss_db = FAISS.from_documents(chunks, hf_emb)\n",
        "\n",
        "def dense_retrieve(q: str, k: int = EVAL_K):\n",
        "    return faiss_db.similarity_search(q, k=k)\n",
        "\n",
        "def hybrid_retrieve(q: str, k: int = EVAL_K):\n",
        "    # simple, stable merge: prefer dense rank, then fill with bm25 uniques\n",
        "    bm_docs = bm25.get_relevant_documents(q)\n",
        "    de_docs = dense_retrieve(q, k=max(k, len(bm_docs)))\n",
        "    seen = set()\n",
        "    merged = []\n",
        "    for d in de_docs:\n",
        "        key = (d.metadata.get(\"source\"), d.metadata.get(\"id\"))\n",
        "        if key not in seen:\n",
        "            merged.append(d)\n",
        "            seen.add(key)\n",
        "        if len(merged) == k:\n",
        "            return merged\n",
        "    for d in bm_docs:\n",
        "        key = (d.metadata.get(\"source\"), d.metadata.get(\"id\"))\n",
        "        if key not in seen:\n",
        "            merged.append(d)\n",
        "            seen.add(key)\n",
        "        if len(merged) == k:\n",
        "            break\n",
        "    return merged[:k]\n",
        "\n",
        "def timed(fn, *args, **kwargs):\n",
        "    t0 = time.time()\n",
        "    out = fn(*args, **kwargs)\n",
        "    t1 = time.time()\n",
        "    return out, (t1 - t0) * 1000.0\n",
        "\n",
        "# -----------------------\n",
        "# golden dataset helpers\n",
        "# -----------------------\n",
        "\n",
        "import regex as re\n",
        "\n",
        "def normalize_txt(s: str) -> str:\n",
        "    # normalize unicode, collapse whitespace, remove hyphen linebreaks\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = s.replace(\"-\\n\", \"\")\n",
        "    s = s.replace(\"\\n\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "id_to_text = {d.metadata[\"id\"]: d.page_content for d in chunks}\n",
        "id_to_normtext = {cid: normalize_txt(txt) for cid, txt in id_to_text.items()}\n",
        "id_to_src = {d.metadata[\"id\"]: d.metadata.get(\"source\", \"\") for d in chunks}\n",
        "\n",
        "def find_chunks_globally_by_regex(patterns, max_hits=3, flags=re.IGNORECASE, filename_contains=None):\n",
        "    # filename_contains can be a string or list to filter by source filename\n",
        "    if isinstance(patterns, str):\n",
        "        patterns = [patterns]\n",
        "    regs = [re.compile(p, flags) for p in patterns]\n",
        "    if filename_contains and isinstance(filename_contains, str):\n",
        "        filename_contains = [filename_contains]\n",
        "    hits = []\n",
        "    for cid, txt in id_to_normtext.items():\n",
        "        src = id_to_src[cid]\n",
        "        if filename_contains:\n",
        "            if not any(k.lower() in src.lower() for k in filename_contains):\n",
        "                continue\n",
        "        for rg in regs:\n",
        "            if rg.search(txt):\n",
        "                hits.append(cid)\n",
        "                break\n",
        "        if len(hits) >= max_hits:\n",
        "            break\n",
        "    return hits\n",
        "\n",
        "golden: List[Dict] = []\n",
        "\n",
        "def add_gold(question: str, answer: str, ground_truth_doc_ids: List[str]):\n",
        "    missing = [i for i in ground_truth_doc_ids if i not in id_to_text]\n",
        "    if missing:\n",
        "        raise ValueError(f\"these ids are not in chunks: {missing}\")\n",
        "    entry = {\n",
        "        \"question\": question.strip(),\n",
        "        \"answer\": answer.strip(),\n",
        "        \"ground_truth_doc_ids\": ground_truth_doc_ids\n",
        "    }\n",
        "    golden.append(entry)\n",
        "    return entry\n",
        "\n",
        "def preview_gold(n: int = 5):\n",
        "    return deepcopy(golden[:n])\n",
        "\n",
        "# -----------------------\n",
        "# build a clean golden set for ai bills only\n",
        "# -----------------------\n",
        "\n",
        "# file name anchors for precision\n",
        "FN_SBN25 = \"20250725 SBN 25 AI Regulation Act.pdf\"\n",
        "FN_SB29  = \"SB 29 - AI Regulation Act - Sen Pia.pdf\"\n",
        "FN_HB2186 = \"HB02186.pdf\"\n",
        "FN_HB7913 = \"HB07913.pdf\"\n",
        "FN_HB10944 = \"HB10944.pdf\"  # present in folder if provided\n",
        "\n",
        "# regex patterns for common sections\n",
        "pat_short_title = [r\"\\bshort\\s+title\\b\", r\"\\bthis\\s+act\\s+shall\\s+be\\s+known\\s+as\\b\", r\"\\bshall\\s+be\\s+known\\s+as\\b\"]\n",
        "pat_policy = [r\"\\bdeclaration\\s+of\\s+policy\\b\", r\"\\bit\\s+is\\s+the\\s+policy\\s+of\\s+the\\s+state\\b\"]\n",
        "pat_objectives = [r\"\\bobjectives?\\b\", r\"\\bgoals?\\b\"]\n",
        "pat_coverage = [r\"\\bcoverage\\b\", r\"\\bscope\\b\"]\n",
        "pat_definitions = [r\"\\bdefinition[s]?\\s+of\\s+terms\\b\", r\"\\bfor\\s+the\\s+purposes?\\s+of\\s+this\\s+act\\b\"]\n",
        "pat_effectivity = [r\"\\bshall\\s+take\\s+effect\\b\", r\"\\beffectivity\\b\"]\n",
        "pat_separability = [r\"\\bseparability\\s+clause\\b\"]\n",
        "pat_repealing = [r\"\\brepealing\\s+clause\\b\"]\n",
        "pat_appropriations = [r\"\\bappropriation[s]?\\b\"]\n",
        "# bill-specific patterns\n",
        "pat_naic = [r\"\\bnational\\s+ai\\s+commission\\b\", r\"\\bnaic\\b\"]\n",
        "pat_aib = [r\"\\bartificial\\s+intelligence\\s+board\\b\", r\"\\baib\\b\"]\n",
        "pat_penalties = [r\"\\bpenalt(y|ies)\\b\", r\"\\badministrative\\s+sanctions?\\b\", r\"\\bviolations?\\b\"]\n",
        "\n",
        "def first_or_none(lst): \n",
        "    return lst[0:1]\n",
        "\n",
        "# sbn 25\n",
        "s_sbn25_title = first_or_none(find_chunks_globally_by_regex(pat_short_title, filename_contains=FN_SBN25))\n",
        "s_sbn25_policy = first_or_none(find_chunks_globally_by_regex(pat_policy, filename_contains=FN_SBN25))\n",
        "s_sbn25_obj = first_or_none(find_chunks_globally_by_regex(pat_objectives, filename_contains=FN_SBN25))\n",
        "s_sbn25_cov = first_or_none(find_chunks_globally_by_regex(pat_coverage, filename_contains=FN_SBN25))\n",
        "s_sbn25_defs = first_or_none(find_chunks_globally_by_regex(pat_definitions, filename_contains=FN_SBN25))\n",
        "s_sbn25_naic = first_or_none(find_chunks_globally_by_regex(pat_naic, filename_contains=FN_SBN25))\n",
        "s_sbn25_pen = first_or_none(find_chunks_globally_by_regex(pat_penalties, filename_contains=FN_SBN25))\n",
        "s_sbn25_eff = first_or_none(find_chunks_globally_by_regex(pat_effectivity, filename_contains=FN_SBN25))\n",
        "s_sbn25_sep = first_or_none(find_chunks_globally_by_regex(pat_separability, filename_contains=FN_SBN25))\n",
        "s_sbn25_rep = first_or_none(find_chunks_globally_by_regex(pat_repealing, filename_contains=FN_SBN25))\n",
        "\n",
        "# sb 29\n",
        "s_sb29_title = first_or_none(find_chunks_globally_by_regex(pat_short_title, filename_contains=FN_SB29))\n",
        "s_sb29_policy = first_or_none(find_chunks_globally_by_regex(pat_policy, filename_contains=FN_SB29))\n",
        "s_sb29_obj = first_or_none(find_chunks_globally_by_regex(pat_objectives, filename_contains=FN_SB29))\n",
        "s_sb29_cov = first_or_none(find_chunks_globally_by_regex(pat_coverage, filename_contains=FN_SB29))\n",
        "s_sb29_defs = first_or_none(find_chunks_globally_by_regex(pat_definitions, filename_contains=FN_SB29))\n",
        "s_sb29_naic = first_or_none(find_chunks_globally_by_regex(pat_naic, filename_contains=FN_SB29))\n",
        "s_sb29_pen = first_or_none(find_chunks_globally_by_regex(pat_penalties, filename_contains=FN_SB29))\n",
        "s_sb29_eff = first_or_none(find_chunks_globally_by_regex(pat_effectivity, filename_contains=FN_SB29))\n",
        "s_sb29_sep = first_or_none(find_chunks_globally_by_regex(pat_separability, filename_contains=FN_SB29))\n",
        "s_sb29_rep = first_or_none(find_chunks_globally_by_regex(pat_repealing, filename_contains=FN_SB29))\n",
        "\n",
        "# hb 2186 (ai education)\n",
        "h_2186_title = first_or_none(find_chunks_globally_by_regex(pat_short_title, filename_contains=FN_HB2186))\n",
        "h_2186_policy = first_or_none(find_chunks_globally_by_regex(pat_policy, filename_contains=FN_HB2186))\n",
        "h_2186_obj = first_or_none(find_chunks_globally_by_regex([r\"\\bstatement\\s+of\\s+objectives\\b\"] + pat_objectives, filename_contains=FN_HB2186))\n",
        "h_2186_scope = first_or_none(find_chunks_globally_by_regex([r\"\\bscope\\s+and\\s+coverage\\b\"] + pat_coverage, filename_contains=FN_HB2186))\n",
        "h_2186_curriculum = first_or_none(find_chunks_globally_by_regex([r\"\\bcurriculum\\s+development\\b\"], filename_contains=FN_HB2186))\n",
        "h_2186_teacher = first_or_none(find_chunks_globally_by_regex([r\"\\bteacher\\s+training\\b\", r\"\\bcapacity[-\\s]?building\\b\"], filename_contains=FN_HB2186))\n",
        "h_2186_eff = first_or_none(find_chunks_globally_by_regex(pat_effectivity, filename_contains=FN_HB2186))\n",
        "\n",
        "# hb 7913 (council + ai board)\n",
        "h_7913_title = first_or_none(find_chunks_globally_by_regex(pat_short_title, filename_contains=FN_HB7913))\n",
        "h_7913_policy = first_or_none(find_chunks_globally_by_regex(pat_policy, filename_contains=FN_HB7913))\n",
        "h_7913_defs = first_or_none(find_chunks_globally_by_regex(pat_definitions, filename_contains=FN_HB7913))\n",
        "h_7913_council = first_or_none(find_chunks_globally_by_regex([r\"\\bphilippine\\s+council\\s+on\\s+artificial\\s+intelligence\\b\"], filename_contains=FN_HB7913))\n",
        "h_7913_aib = first_or_none(find_chunks_globally_by_regex(pat_aib, filename_contains=FN_HB7913))\n",
        "h_7913_wholegov = first_or_none(find_chunks_globally_by_regex([r\"\\bwhole\\s+of\\s+government\\s+approach\\b\"], filename_contains=FN_HB7913))\n",
        "h_7913_database = first_or_none(find_chunks_globally_by_regex([r\"\\bcentral\\s+database\\b\"], filename_contains=FN_HB7913))\n",
        "h_7913_pen = first_or_none(find_chunks_globally_by_regex(pat_penalties, filename_contains=FN_HB7913))\n",
        "h_7913_eff = first_or_none(find_chunks_globally_by_regex(pat_effectivity, filename_contains=FN_HB7913))\n",
        "\n",
        "# hb 10944 optional, only if present in folder\n",
        "h_10944_title = first_or_none(find_chunks_globally_by_regex(pat_short_title, filename_contains=FN_HB10944)) if any(FN_HB10944 in s for s in id_to_src.values()) else []\n",
        "h_10944_policy = first_or_none(find_chunks_globally_by_regex(pat_policy, filename_contains=FN_HB10944)) if h_10944_title else []\n",
        "\n",
        "def add_if_found(q: str, cids: List[str]):\n",
        "    # use the located ground truth as the answer text for ragas stability\n",
        "    if not cids:\n",
        "        print(f\"note: no ground truth found for '{q}'\")\n",
        "        return\n",
        "    gt_texts = [id_to_text[cids[0]]]\n",
        "    add_gold(q, gt_texts[0][:3000], cids)  # keep answer non-empty but trimmed\n",
        "\n",
        "# sbn 25 questions\n",
        "add_if_found(\"what is the short title of sbn 25?\", s_sbn25_title)\n",
        "add_if_found(\"what is the declaration of policy under sbn 25?\", s_sbn25_policy)\n",
        "add_if_found(\"what are the objectives under sbn 25?\", s_sbn25_obj)\n",
        "add_if_found(\"what is the coverage of sbn 25?\", s_sbn25_cov)\n",
        "add_if_found(\"what are the definitions under sbn 25?\", s_sbn25_defs)\n",
        "add_if_found(\"what body does sbn 25 create to oversee ai?\", s_sbn25_naic)\n",
        "add_if_found(\"are there penalties or administrative sanctions in sbn 25?\", s_sbn25_pen)\n",
        "add_if_found(\"what is the effectivity clause of sbn 25?\", s_sbn25_eff)\n",
        "add_if_found(\"what is the separability clause of sbn 25?\", s_sbn25_sep)\n",
        "add_if_found(\"what is the repealing clause of sbn 25?\", s_sbn25_rep)\n",
        "\n",
        "# sb 29 questions\n",
        "add_if_found(\"what is the short title of sb 29?\", s_sb29_title)\n",
        "add_if_found(\"what is the declaration of policy under sb 29?\", s_sb29_policy)\n",
        "add_if_found(\"what are the objectives under sb 29?\", s_sb29_obj)\n",
        "add_if_found(\"what is the coverage of sb 29?\", s_sb29_cov)\n",
        "add_if_found(\"what are the definitions under sb 29?\", s_sb29_defs)\n",
        "add_if_found(\"what body does sb 29 create to regulate ai?\", s_sb29_naic)\n",
        "add_if_found(\"are there penalties or administrative sanctions in sb 29?\", s_sb29_pen)\n",
        "add_if_found(\"what is the effectivity clause of sb 29?\", s_sb29_eff)\n",
        "add_if_found(\"what is the separability clause of sb 29?\", s_sb29_sep)\n",
        "add_if_found(\"what is the repealing clause of sb 29?\", s_sb29_rep)\n",
        "\n",
        "# hb 2186 questions\n",
        "add_if_found(\"what is the short title of hb 2186?\", h_2186_title)\n",
        "add_if_found(\"what is the declaration of policy under hb 2186?\", h_2186_policy)\n",
        "add_if_found(\"what are the objectives of hb 2186?\", h_2186_obj)\n",
        "add_if_found(\"what is the scope and coverage of hb 2186?\", h_2186_scope)\n",
        "add_if_found(\"what is covered in curriculum development under hb 2186?\", h_2186_curriculum)\n",
        "add_if_found(\"what are the teacher training provisions under hb 2186?\", h_2186_teacher)\n",
        "add_if_found(\"what is the effectivity clause of hb 2186?\", h_2186_eff)\n",
        "\n",
        "# hb 7913 questions\n",
        "add_if_found(\"what is the short title of hb 7913?\", h_7913_title)\n",
        "add_if_found(\"what is the declaration of policy under hb 7913?\", h_7913_policy)\n",
        "add_if_found(\"what are the definitions under hb 7913?\", h_7913_defs)\n",
        "add_if_found(\"what is the philippine council on ai in hb 7913?\", h_7913_council)\n",
        "add_if_found(\"what is the artificial intelligence board in hb 7913?\", h_7913_aib)\n",
        "add_if_found(\"what is the whole of government approach in hb 7913?\", h_7913_wholegov)\n",
        "add_if_found(\"what is the central database requirement in hb 7913?\", h_7913_database)\n",
        "add_if_found(\"are there penalties or violations listed in hb 7913?\", h_7913_pen)\n",
        "add_if_found(\"what is the effectivity clause of hb 7913?\", h_7913_eff)\n",
        "\n",
        "# hb 10944 optional samples\n",
        "if h_10944_title:\n",
        "    add_if_found(\"what is the short title of hb 10944?\", h_10944_title)\n",
        "    add_if_found(\"what is the declaration of policy under hb 10944?\", h_10944_policy)\n",
        "\n",
        "print(\"golden size now:\", len(golden))\n",
        "print(\"golden preview:\", preview_gold(5))\n",
        "\n",
        "# -----------------------\n",
        "# ragas metrics shim and dataset builder\n",
        "# -----------------------\n",
        "\n",
        "METRICS = None\n",
        "NAMES = None\n",
        "\n",
        "try:\n",
        "    # older function api\n",
        "    from ragas.metrics import context_precision, context_recall, context_relevancy\n",
        "    METRICS = [context_precision, context_recall, context_relevancy]\n",
        "    NAMES = [\"context_precision\", \"context_recall\", \"context_relevancy\"]\n",
        "except Exception:\n",
        "    try:\n",
        "        # newer class api\n",
        "        from ragas.metrics import ContextPrecision, ContextRecall, ContextRelevancy\n",
        "        METRICS = [ContextPrecision(), ContextRecall(), ContextRelevancy()]\n",
        "        NAMES = [\"context_precision\", \"context_recall\", \"context_relevancy\"]\n",
        "    except Exception:\n",
        "        from ragas.metrics import context_precision, context_recall\n",
        "        METRICS = [context_precision, context_recall]\n",
        "        NAMES = [\"context_precision\", \"context_recall\"]\n",
        "\n",
        "from ragas import evaluate\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "def build_eval_dataset(\n",
        "    golden_items: List[Dict],\n",
        "    k: int,\n",
        "    retrieve_fn: Callable[[str, int], List[Document]],\n",
        "):\n",
        "    # ragas expects: question, contexts (list[str]), ground_truths (list[str]), answer, reference\n",
        "    rows = []\n",
        "    for row in golden_items:\n",
        "        q = row[\"question\"]\n",
        "        hits = retrieve_fn(q, k=k) or []\n",
        "        contexts = [d.page_content for d in hits]\n",
        "\n",
        "        # build ground truth texts from the saved ids\n",
        "        gts = [id_to_text[i] for i in row[\"ground_truth_doc_ids\"] if i in id_to_text]\n",
        "\n",
        "        # use gt as both answer and reference to keep api differences happy\n",
        "        answer = gts[0] if gts else row.get(\"answer\", \"\")\n",
        "        reference = \"\\n\\n\".join(gts) if gts else answer\n",
        "\n",
        "        rows.append({\n",
        "            \"question\": q,\n",
        "            \"contexts\": contexts,\n",
        "            \"ground_truths\": gts if gts else [reference] if reference else [],\n",
        "            \"ground_truth\": gts[0] if gts else reference,\n",
        "            \"answer\": answer,\n",
        "            \"reference\": reference,\n",
        "        })\n",
        "    return Dataset.from_list(rows)\n",
        "\n",
        "def eval_retriever(golden_items, name: str, retrieve_fn, k: int = EVAL_K, n_latency: int = 10):\n",
        "    ds = build_eval_dataset(golden_items, k=k, retrieve_fn=retrieve_fn)\n",
        "    report = evaluate(ds, metrics=METRICS)\n",
        "\n",
        "    # robust getter across ragas versions\n",
        "    def get_metric(rep, key):\n",
        "        try:\n",
        "            return float(rep[key])\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            return float(getattr(rep, \"scores\", {}).get(key))\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            df = rep.to_pandas()\n",
        "            if key in df.columns:\n",
        "                return float(df[key].iloc[0])\n",
        "        except Exception:\n",
        "            pass\n",
        "        return None\n",
        "\n",
        "    vals = {k: get_metric(report, k) for k in NAMES}\n",
        "\n",
        "    # latency probe\n",
        "    times = []\n",
        "    for row in golden_items[:max(1, min(n_latency, len(golden_items)))]:\n",
        "        t0 = time.time()\n",
        "        _ = retrieve_fn(row[\"question\"], k)\n",
        "        t1 = time.time()\n",
        "        times.append((t1 - t0) * 1000.0)\n",
        "    avg_ms = statistics.mean(times) if times else None\n",
        "\n",
        "    out = {\"retriever\": name, \"avg_latency_ms\": round(avg_ms, 1) if avg_ms else None}\n",
        "    out.update({k: (round(v, 3) if v is not None else None) for k, v in vals.items()})\n",
        "    return out\n",
        "\n",
        "# -----------------------\n",
        "# evaluate retrievers (recall-oriented run at k=EVAL_K)\n",
        "# -----------------------\n",
        "\n",
        "def bm25_retrieve(q: str, k: int = EVAL_K):\n",
        "    return bm25.get_relevant_documents(q)[:k]\n",
        "\n",
        "results = []\n",
        "for name, fn in [\n",
        "    (\"bm25\", bm25_retrieve),\n",
        "    (\"dense\", dense_retrieve),\n",
        "    (\"hybrid\", hybrid_retrieve),\n",
        "]:\n",
        "    results.append(\n",
        "        eval_retriever(\n",
        "            golden_items=golden,\n",
        "            name=name,\n",
        "            retrieve_fn=fn,\n",
        "            k=EVAL_K,\n",
        "            n_latency=min(10, len(golden)),\n",
        "        )\n",
        "    )\n",
        "\n",
        "# optional small-chunk comparison\n",
        "if DO_SMALL_CHUNK_COMPARISON:\n",
        "    splitter_small = RecursiveCharacterTextSplitter(chunk_size=350, chunk_overlap=60)\n",
        "    chunks_small = splitter_small.split_documents(docs)\n",
        "\n",
        "    bm25_small = BM25Retriever.from_documents(chunks_small); bm25_small.k = EVAL_K\n",
        "    faiss_db_small = FAISS.from_documents(chunks_small, hf_emb)\n",
        "\n",
        "    def dense_retrieve_small(q: str, k: int = EVAL_K):\n",
        "        return faiss_db_small.similarity_search(q, k=k)\n",
        "\n",
        "    def hybrid_retrieve_small(q: str, k: int = EVAL_K):\n",
        "        de_docs = dense_retrieve_small(q, k=max(k, 16))\n",
        "        bm_docs = bm25_small.get_relevant_documents(q)\n",
        "        seen = set(); merged = []\n",
        "        for d in de_docs:\n",
        "            key = (d.metadata.get(\"source\"), d.metadata.get(\"id\"))\n",
        "            if key not in seen:\n",
        "                merged.append(d); seen.add(key)\n",
        "            if len(merged) == k: return merged\n",
        "        for d in bm_docs:\n",
        "            key = (d.metadata.get(\"source\"), d.metadata.get(\"id\"))\n",
        "            if key not in seen:\n",
        "                merged.append(d); seen.add(key)\n",
        "            if len(merged) == k: break\n",
        "        return merged[:k]\n",
        "\n",
        "    for name, fn in [\n",
        "        (\"bm25_smallchunk\", lambda q, k=EVAL_K: bm25_small.get_relevant_documents(q)[:k]),\n",
        "        (\"dense_smallchunk\", dense_retrieve_small),\n",
        "        (\"hybrid_smallchunk\", hybrid_retrieve_small),\n",
        "    ]:\n",
        "        results.append(\n",
        "            eval_retriever(\n",
        "                golden_items=golden,\n",
        "                name=name,\n",
        "                retrieve_fn=fn,\n",
        "                k=EVAL_K,\n",
        "                n_latency=min(10, len(golden)),\n",
        "            )\n",
        "        )\n",
        "\n",
        "# -----------------------\n",
        "# precision-oriented second run at k=EVAL_K_PREC\n",
        "# -----------------------\n",
        "\n",
        "results_lowk = []\n",
        "for name, fn in [\n",
        "    (\"bm25_k2\", lambda q, k=EVAL_K_PREC: bm25.get_relevant_documents(q)[:k]),\n",
        "    (\"dense_k2\", lambda q, k=EVAL_K_PREC: dense_retrieve(q, k=EVAL_K_PREC)),\n",
        "    (\"hybrid_k2\", lambda q, k=EVAL_K_PREC: hybrid_retrieve(q, k=EVAL_K_PREC)),\n",
        "]:\n",
        "    results_lowk.append(\n",
        "        eval_retriever(\n",
        "            golden_items=golden,\n",
        "            name=name,\n",
        "            retrieve_fn=fn,\n",
        "            k=EVAL_K_PREC,\n",
        "            n_latency=min(10, len(golden)),\n",
        "        )\n",
        "    )\n",
        "\n",
        "# -----------------------\n",
        "# build results tables and summaries\n",
        "# -----------------------\n",
        "\n",
        "from pandas import DataFrame\n",
        "\n",
        "cols = [\"retriever\"] + NAMES + [\"avg_latency_ms\"]\n",
        "\n",
        "df = DataFrame(results)\n",
        "df = df[[c for c in cols if c in df.columns]]\n",
        "sort_key = \"context_recall\" if \"context_recall\" in df.columns else (NAMES[0] if len(NAMES) else None)\n",
        "if sort_key:\n",
        "    df = df.sort_values(by=sort_key, ascending=False)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "df_lowk = DataFrame(results_lowk)\n",
        "df_lowk = df_lowk[[c for c in cols if c in df_lowk.columns]]\n",
        "if sort_key and sort_key in df_lowk.columns:\n",
        "    df_lowk = df_lowk.sort_values(by=sort_key, ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n=== retriever comparison (k = EVAL_K) ===\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "print(\"\\n=== retriever comparison (k = EVAL_K_PREC) ===\")\n",
        "print(df_lowk.to_string(index=False))\n",
        "\n",
        "def quick_summary(df_in: DataFrame):\n",
        "    for _, row in df_in.iterrows():\n",
        "        name = row[\"retriever\"]\n",
        "        lat = row.get(\"avg_latency_ms\", None)\n",
        "        parts = []\n",
        "        for m in [\"context_precision\", \"context_recall\", \"context_relevancy\"]:\n",
        "            if m in df_in.columns and m in row:\n",
        "                parts.append(f\"{m}={row[m]}\")\n",
        "        metrics_str = \", \".join(parts) if parts else \"no ragas metrics found\"\n",
        "        print(f\"- {name}: {metrics_str} | avg_latency_ms={lat}\")\n",
        "\n",
        "print(\"\\n=== quick summary (k = EVAL_K) ===\")\n",
        "quick_summary(df)\n",
        "\n",
        "print(\"\\n=== quick summary (k = EVAL_K_PREC) ===\")\n",
        "quick_summary(df_lowk)\n",
        "\n",
        "# end of script\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "source": [
        "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
        "\n",
        "### Analysis & Observations:\n",
        "\n",
        "Cost\n",
        "\n",
        "* BM25 & BM25 Smallchunk\n",
        "* Extremely low latency (~0.6–0.9 ms), meaning they’re very cheap to run.\n",
        "* No external embedding model calls, so zero token-based cost.\n",
        "* Dense & Hybrid Models\n",
        "* Much higher latency (~13–17 ms) due to embedding lookups and similarity search.\n",
        "* Embedding-based retrievers incur token embedding costs if not cached.\n",
        "* Hybrid combines both costs (BM25 + dense) without significant latency savings.\n",
        "\n",
        "Latency\n",
        "* Fastest: BM25 variants (<1 ms) — ideal for real-time, cost-sensitive queries.\n",
        "* Slowest: Dense/Hybrid smallchunk (~16–17 ms) — latency could stack if chaining multiple retrievals per user query.\n",
        "* Trade-off: Higher-latency retrievers generally bring semantic matching advantages, but here recall issues reduce that benefit.\n",
        "\n",
        "Performance\n",
        "\n",
        "BM25 (Regular)\n",
        "\n",
        "* Context Precision: 0.415 (lower precision, more noise)\n",
        "\n",
        "* Context Recall: 1.0 (perfect recall in current small golden set)\n",
        "\n",
        "* Good for coverage but needs better ranking for precision.\n",
        "\n",
        "Dense & Hybrid (Regular)\n",
        "\n",
        "* Higher precision (0.526–0.594) but 0.0 recall — indicating poor match rate to gold set due to vocabulary or semantic mismatch.\n",
        "\n",
        "* Strength is underutilized due to small and literal golden dataset.\n",
        "\n",
        "Smallchunk Variants\n",
        "\n",
        "* Very high precision (0.800–1.0) but still 0.0 recall — meaning they find exact matches very accurately but miss most gold answers.\n",
        "\n",
        "* Works for pinpoint fact retrieval, but risky for broad coverage.\n",
        "\n",
        "Key Observation\n",
        "\n",
        "* Current dataset size and missing gold entries skew recall scores — BM25 appears to dominate recall only because the golden set aligns well with keyword matching.\n",
        "* Dense/hybrid retrievers are underperforming not because they’re weak, but because the queries and gold answers don’t match semantically enough for vector search to trigger.\n",
        "* Smallchunk boosts precision significantly, but without improved recall, it’s not viable alone for coverage-heavy use cases.\n",
        "\n",
        "\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
